                                                                               MACHINE LEARNING – WORKSHEET 11
                                                                                      (LINEAR REGRESSION)


1. (C) may or may not increase

2. (B) SST = SSR + SSE

3. (A) difference between the actual value and the predicted value.

4. (C) By its slope

5. (B) can be either -1 or 1

6. (A) Scatter plot

7. (B) f-statistics

8. (C) Ridge

9. (A) It shows the causal relationship between dependent and independent variables
   (C) It always goes through origin
   (D) It is a straight line that is the best approximation of the given data sets

10. (A) Reducing the training time
    (B) Generalizing the test set
    (C) Automatic feature selection

11. (A) Normal Equation
    (B) Singular Value Decomposition
    (C) Parity checks
    (D) nodes

12. R-squared (R2), which is the proportion of variation in the outcome that is explained by the predictor variables. In multiple regression models, R2 corresponds to the squared correlation
    between the observed outcome values and the predicted values by the model. The Higher the R-squared, the better the model.

    R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination
    for multiple regression. 100% indicates that the model explains all the variability of the response data around its mean.

    The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the 
    model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.

13. A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference or distance between the 
    predicted value and the actual value. The cost function (we may also see this referred to as loss or error) can be estimated by iteratively running the model to compare estimated predictions 
    against “ground truth” — the known values of y.

14. Total sum of squares (SST) is the sum of squared deviations of individual measurements from the mean. The total sum of squares is a sum of 2 portions:

    (a) Regression sum of squares (SSR) which is the contribution of factors into the variance of the dependent variable, and

    (b) Error sum of squares (=residual sum of squares) (SSE) which is the stochastic component of the variation of the dependent variable.

    SSR is the sum of squared deviations of predicted values (predicted using regression) from the mean value, and SSE is the sum of squared deviations of actual values from predicted values.

    The significance of regression is evaluated using F-statistics:

    F = [SSR. df(SSE)] / [SSE. df(SSR)]

      = [SSR / df(SSR)] / [SSE / df(SSE)]

    Where,
  
    df(SSR)= g - 1 is the number of degrees of freedom for the regression sum of squares which is equal to the number of coefficients in the equation, g, minus 1;

    df(SSE)= N - g is the number of degrees of freedom for the error sum of squares which is equal to the number of observations, N, minus the number of coefficients, g;

    df(SST) = df(SSR) + df(SSE) = N - 1 is the number of degrees of freedom for the total sum of squares

    
    Mathematically, SST = SSR + SSE.

15. The various evaluation metrics for linear regression are :-

    (a) r2_score

    (b) Mean Squared Error (MSE)

    (c) Mean Absolute Error (MAE)

    (d) Root Mean Squared Error (RMSE) R-Squared




