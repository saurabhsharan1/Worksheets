                                                                                    Statistics– WORKSHEET 6

                                                                                             ANSWERS


1. (b) Total Variation = Residual Variation + Regression Variation

2. (c) binomial

3. (a) 2

4. (a) Type-I error

5. (b) Size of the test

6. (b) Increases

7. (b) Hypothesis

8. (d) All of the mentioned

9. (a) 0

10. In statistics and probability theory, the Bayes’ theorem (also known as the Bayes’ rule) is a mathematical formula used to determine the conditional probability of events. 
    Essentially, the Bayes’ theorem describes the probability of an event based on prior knowledge of the conditions that might be relevant to the event.

    The theorem is named after English statistician, Thomas Bayes, who discovered the formula in 1763. It is considered the foundation of the special statistical inference approach 
    called the Bayes’ inference.
  
     
                    P(B|A)* P(A)
    P(A|B) =       ______________

                        P(B)

    
    Where:

    * P(A|B) – the probability of event A occurring, given event B has occurred

    * P(B|A) – the probability of event B occurring, given event A has occurred

    * P(A) – the probability of event A

    * P(B) – the probability of event B

    Besides statistics, the Bayes’ theorem is also used in various disciplines, with medicine and pharmacology as the most notable examples. In addition, the theorem is commonly employed
    in different fields of finance. Some of the applications include but are not limited to, modeling the risk of lending money to borrowers or forecasting the probability of the success 
    of an investment. 


11.  Simply, a z-score (also called a standard score) gives an idea of how far from the mean a data point is. But more technically it’s a measure of how many standard deviations below or 
     above the population mean a raw score is.

     A z-score can be placed on a normal distribution curve. Z-scores range from -3 standard deviations (which would fall to the far left of the normal distribution curve) up to +3 standard 
     deviations (which would fall to the far right of the normal distribution curve). In order to use a z-score, we need to know the mean μ and also the population standard deviation σ.

     Z-scores are a way to compare results to a “normal” population. Results from tests or surveys have thousands of possible results and units; those results can often seem meaningless. For
     example, knowing that someone’s weight is 150 pounds might be good information, but if we want to compare it to the “average” person’s weight, looking at a vast table of data can be overwhelming 
     (especially if some weights are recorded in kilograms). A z-score can tell us where that person’s weight is compared to the average population’s mean weight.

     The basic z score formula for a sample is:

     z = (x – μ) / σ

     For example, let’s say we have a test score of 190. The test has a mean (μ) of 150 and a standard deviation (σ) of 25. Assuming a normal distribution, the z score would be:

     * z = (x – μ) / σ

         = (190 – 150) / 25 = 1.6.

     The z score tells us how many standard deviations from the mean our score is. In this example, our score is 1.6 standard deviations above the mean.


12.  A t-test is a type of inferential statistic used to determine if there is a significant difference between the means of two groups, which may be related in certain features. It is mostly 
     used when the data sets, like the data set recorded as the outcome from flipping a coin 100 times, would follow a normal distribution and may have unknown variances. A t-test is used as a
     hypothesis testing tool, which allows testing of an assumption applicable to a population. 

     A t-test looks at the t-statistic, the t-distribution values, and the degrees of freedom to determine the statistical significance. To conduct a test with three or more means, one must use 
     an analysis of variance.


13.  A percentile is a term used in statistics to express how a score compares to other scores in the same set. While there is technically no standard definition of percentile, it's typically 
     communicated as the percentage of values that fall below a particular value in a set of data scores.

     Percentiles are commonly used to report values from norm-referenced tests (in which the average is determined by comparing a set of results in the same group) as the percentages of scores 
     that fall below those of the average of the set. For example, a male child age 12 with a weight of 130 pounds is at the 90th percentile of weight for males of that age, which indicates that 
     he weighs more than 90 percent of other 12-year-old boys.

     In statistical terms, there are three separate definitions of percentile. They are:

     * Greater than: The kth percentile is the lowest score in a data set that is greater than a percentage (k) of the scores. For example, if k = .25, we would be trying to identify the lowest score
       that is greater than 25% of scores in the data set.

     * Greater than or equal to: The k**th percentile is the lowest score in the data set that is greater than or equal to a percentage (k) of the scores. For example, if k = .25, we would be looking 
       for a value that is greater or equal to 25% of the scores.

     * Weighted average: In this method, the kth percentile is the weighted average of the percentiles calculated in the two definitions above. This method allows for numbers to be more neatly rounded 
       and defines the median of the set as the 50th percentile.


14.  Analysis of variance (ANOVA) is an analysis tool used in statistics that splits an observed aggregate variability found inside a data set into two parts: systematic factors and random factors. The 
     systematic factors have a statistical influence on the given data set, while the random factors do not. Analysts use the ANOVA test to determine the influence that independent variables have on the 
     dependent variable in a regression study.

     The t- and z-test methods developed in the 20th century were used for statistical analysis until 1918, when Ronald Fisher created the analysis of variance method.

     ANOVA is also called the Fisher analysis of variance, and it is the extension of the t- and z-tests. The term became well-known in 1925, after appearing in Fisher's book, "Statistical Methods for Research 
     Workers." It was employed in experimental psychology and later expanded to subjects that were more complex.

     The Formula for ANOVA is:

             MST 
     F  =   _____ 

             MSE 


     where:
 
     * F=ANOVA coefficient

     * MST = Mean sum of squares due to treatment

     * MSE = Mean sum of squares due to error



15.  An ANOVA test is a way to find out if survey or experiment results are significant. In other words, they help us to figure out if we need to reject the null hypothesis or accept the alternate hypothesis.
     Basically, we are testing groups to see if there’s a difference between them. Examples of when we might want to test different groups:

     * A group of psychiatric patients are trying three different therapies: counseling, medication and biofeedback. We want to see if one therapy is better than the others.

     * A manufacturer has two different processes to make light bulbs. They want to know if one process is better than the other.

     * Students from different colleges take the same exam. We want to see if one college outperforms the other.


     The one-way ANOVA can help us to know whether or not there are significant differences between the means of our independent variables (such as the first example: age, sex, income). When we understand
     how each independent variable’s mean is different from the others, we can begin to understand which of them has a connection to our dependent variable (landing page clicks), and begin to learn what is
     driving that behavior.

